{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = '../data.csv'  # Adjust path if necessary\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocessing: Keep only relevant columns and remove null values\n",
    "df = df[['text']]  # Focus on the 'text' column\n",
    "df.dropna(subset=['text'], inplace=True)  # Remove rows with null 'text'\n",
    "\n",
    "# Reduce the dataset size to 60,000 rows (use first 60k rows or a random sample)\n",
    "df = df.head(60000)  # Keep only the first 60,000 rows\n",
    "\n",
    "# If you want to select a random sample of 60,000 rows, use:\n",
    "# df = df.sample(n=60000, random_state=42)  # Use random_state for reproducibility\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Print first few rows to verify the data\n",
    "print(\"Dataset loaded and preprocessed. Sample data:\\n\", df.head())\n",
    "\n",
    "print(\"Dataset Length: \",len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Set padding token to the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print a sample tokenized data\n",
    "print(\"Tokenization complete. Sample tokenized data:\\n\", tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "train_dataset, val_dataset = tokenized_dataset.train_test_split(test_size=0.2).values()\n",
    "\n",
    "# Print split info\n",
    "print(f\"Data split into {len(train_dataset)} training samples and {len(val_dataset)} validation samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load distilGPT-2 model for the teacher model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "freeze_layers = 3  # Freezing first 3 layers\n",
    "for i, layer in enumerate(model.transformer.h):\n",
    "    if i < freeze_layers:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False  # Freeze layer\n",
    "\n",
    "# Ensure the final layers and output layers remain trainable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print model loading confirmation\n",
    "print(\"DistilGPT-2 model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    # We need to get the predictions and labels from the output\n",
    "    logits, labels = p\n",
    "    # Use the softmax function to convert logits to probabilities (this is for classification tasks)\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Flatten predictions and labels for evaluation\n",
    "    predictions = predictions.flatten().cpu().numpy()\n",
    "    labels = labels.flatten().cpu().numpy()\n",
    "\n",
    "    # Compute perplexity (standard for language models)\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    perplexity = torch.exp(loss).item()\n",
    "\n",
    "    # Print perplexity for tracking\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "\n",
    "    # Return the dictionary of metrics\n",
    "    return {\n",
    "        'perplexity': perplexity,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily reduce the number of epochs for testing\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_shakespeare\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,  # Reduced batch size\n",
    "    per_device_eval_batch_size=16,   # Reduced batch size\n",
    "    num_train_epochs=3,  # Reduce epochs to 3 temporarily\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    fp16=False,  # Disable mixed precision for CPU training\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize DataCollator for Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to False for causal language modeling (GPT-2)\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator  # Use the DataCollator here\n",
    ")\n",
    "\n",
    "# Start training and track progress\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "# Training loop with progress\n",
    "trainer.train()\n",
    "\n",
    "# Confirm completion of training\n",
    "print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained teacher model\n",
    "model.save_pretrained(\"./gpt2_shakespeare_teacher\")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Teacher model saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
